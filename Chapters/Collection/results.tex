\section{Corpus Statistics}
Candidates from the detection approaches are considered to be an event if more than 50\% of annotators marked it as so and greater than 90\% of judged tweets were marked as relevant.
This resulted in 382 events for the LSH approach, and 53 for the CS approach.
Candidates from the Wikipedia approach are considered an event if they produced at least 1 relevant tweet, resulting in 361 events and 7 non-events (i.e. candidates where no tweets were marked as relevant).
In total, this resulted in 796 events \emph{before} merging.

Individual tweets are regarded as relevant if more than 50\% of annotators agreed.
Table~\ref{table:eventsByTweets} shows the distribution of tweets broken down by both approach and judgment type (explicit or implicit).
Explicit judgements are those made by human annotators, whereas implicit judgements are tweets from events with high precision (\textgreater 0.9)  however that were not judged by human annotators directly.
This gave 4,009 explicit and 93,398 implicit judged tweets for the LSH approach, 465 explicit and 15,098 implicit judgements for the CS approach, and 39,980 explicit judgements (with no implicit judgements) for the Wikipedia approach.
Although the use of implicit judgements will have introduced some noise to the relevance judgements, because we remove candidates with low precision we are able to minimize noise whilst increasing the number of judgements by over 200\%.

\begin{table}[h!]
	\centering
	\caption[Distribution of relevance judgements across the different approaches]{The distribution of relevance judgements across the different approaches. Explicit judgements are made by human annotators, implicit judgements are taken from events with high precision (\textgreater 0.9) but not judged by human annotators individually.}
	\label{table:eventsByTweets}

	\begin{tabulary}{\textwidth}{l r r r}
	\toprule
	\textbf{Approach} & \textbf{Explicit} & \textbf{Implicit} & \textbf{Total} \\
	\midrule
	LSH 		& 4,009 	& 93,398 	&  97,407 \\
	CS 			& 465 		& 15,098 	&  15,563 \\
	Wikipedia 	& 39,980 	& 0 		&  39,980  \\
	\midrule
	\textbf{Total} 	& \textbf{44,454} & \textbf{108,496} & \textbf{152,950}\\
	\bottomrule
	\end{tabulary}

\end{table}

\subsection{Events After Merging}
After merging, 506 events remain.
In total, 367 events were merged down to 77 events, and a further 429 events were not merged.
The detection approaches contributed to 186 events after merging, while the Wikipedia approach contributed 342, almost double that of the detection method.
However, the detection approaches contribute over 110,000 of the ~150,000 relevance judgements in the corpus, with an average of 259 tweets per event (before merging).
The Wikipedia approach contributes just 39,980 of the relevance judgements, at an average of 85 tweets per event.
This is presumably because of the different types of event identified by each of the methods.
While the detection approaches rely on the volume of tweets to identify events, the Wikipedia approach does not, meaning that it was able to product a much larger set of small events.
The combination of the two approaches allows their different characteristics to complement each other, producing a much more robust corpus than would have been produced had a single approach been used.
If only one approach had been used, then the results would have been unevenly skewed towards large-scale events that are discussed by huge volumes of people, such as sports events or election debates, where as the use of both approaches smooth this out, to better reflect the distribution of events that occur in the real world, rather than what is discussed by more users on Twitter.

It is interesting to note that, although we could have used the number of shared tweets as a feature for clustering, it would have made no difference to the resulting clusters.
Out of 41 cases where events share more than 10 tweets, there is only a single case where they do not have a similarity score above our threshold, and the events are subsequently clustered through shared similarity to a 3rd event.
This helps to demonstrate the effectiveness and robustness of our event merging approach.


\subsection{Event Categories}
Categories are assigned to events based on the combined categories defined in Table~\ref{collection:table:catTable}.
For events where multiple categories where given by annotators (particularly common after events have been merged), the category with the highest frequency was used.
In cases where there was a tie between the categories, an author was used to give the deciding vote.

\begin{table}[]
	\centering

	\caption[The distribution of events across the 8 different categories, broken down by method used]{The distribution of events across the 8 different categories, broken down by method used. The LSH, CS and Wiki columns show numbers of events \emph{before} clustering, while the Clustered column shows the number of events \emph{after} clustering has been performed. }
	\label{table:eventsByCat}

	\begin{tabulary}{\textwidth}{l c c c c}
	\toprule
	\textbf{Category} & \textbf{Merged} & \textbf{LSH} & \textbf{CS} & \textbf{Wiki}  \\
	\midrule

	Armed Conflicts \& Attacks 			& 98 	& 3 	& 1 	& 95 \\
	Arts, Culture \& Entertainment 		& 53 	& 26 	& 3 	& 34 \\
	Business \& Economy 				& 23 	& 2 	& 1 	& 22 \\
	Disasters \& Accidents 				& 29 	& 16 	& 4 	& 23 \\
	Law, Politics and Scandals 			& 140 	& 124 	& 12 	& 128 \\
	Miscellaneous 						& 21	& 26 	& 6 	& 3 \\
	Science and Technology 				& 16 	& 10 	& 2 	& 11 \\
	Sports 								& 126 	& 175 	& 24 	& 26 \\

	\bottomrule
	\end{tabulary}

\end{table}

Table~\ref{table:eventsByCat} gives a breakdown of the number of events per category before and after merging.
Both the detection approaches and the Wikipedia approach produced very different category distributions, however seem to complement each other, and the combined results show much less variation than if only one approach had been used.
As expected, the detection approaches (LSH and CS) seems to closely reflect to the types of events most commonly discussed on Twitter~\cite{zhao2011empirical}, while the Wikipedia approach gives a more realistic representation of real-world events and news.

For example, the detection approach contributes a large number of \emph{Sports} events, something which is lacking from the Wikipedia approach.
Likewise, the Wikipedia approach contributes a large number of events from \emph{Armed Conflicts and Attacks}, and \emph{Business and Economy}, both categories where the detection approach produces less results.
This could be due to the volume of discussion associated with each of these topics.
\emph{Law, Politics and Scandals} have very few tweets per event in comparison with Sports, meaning that restricting the detection approaches to clusters with at least 30 tweets may have removed many which were discussing these types of event.
Since we did not put this restriction in place for the Wikipedia approach, it was not affected by the low volume of discussion, so is able to produce more events of this type.

\subsection{Completeness and Re-usability}
Since the aim of the test collection is to enable the comparable evaluation of event detection approaches, it is necessary to discuss the completeness and re-usability of the collection.
It is a infeasible for the collection to contain relevance judgements for all tweets and for every event.
Even with our definition of event, deciding what constitutes an event is a subjective task, and having multiple annotators read all 120 million tweets in the collection to annotate all events and all relevant tweets is an impossible task.
For that reason, the pooling strategy we employed is one of the few strategies that are ever likely to be effective for building a large scale collection for event detection.
However, the lack of completeness raises an number of evaluation that need to be examined, and if possible, addressed when performing evaluations using this collection.

Since we know that the relevance judgements are incomplete in terms of tweets for each event, and that an event detection system is unlikely to correctly identify every tweet for an event, we must consider how this will affect evaluations.
For example, we cannot expect to ever have full relevance judgements for even a fraction of the millions of tweets that were posted about the US Presidential debates in 2012\footnote{\texttt{http://www.nbcnews.com/technology/presidential-debate-sets-twitter\\-record-6281796}}.
Additionally, because automated methods were used to generate the relevance judgements, each with differing levels of granularity, there are a number of events in the judgements where only part of an event has been detected (for example, a single goal in a football match, rather than the football match itself).
This means that any evaluation using the collection must allow for inexact matching when determining relevance, otherwise it will greatly underestimate effectiveness.

It is easy to show that more than 506 events occurred between the dates the collection covers, meaning that there will certainly be events we have no annotations for.
Any sort of comparative evaluation using this collection must take this into consideration.
However, the collection should contain enough events and relevance judgements to serve as a representative sample, and should still be useful when performing evaluations.
Although precision and recall are likely to be underestimated, results should still be comparable, particularity between different runs of the same approach during development and testing.
A crowdsourced evaluation, using the methodology proposed in this chapter, could then be used to perform a through and more accurate evaluation.

