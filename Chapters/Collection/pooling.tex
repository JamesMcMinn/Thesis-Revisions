%!TEX root = ../../main.tex

\section{TREC Pooling \& Crowdsourcing}
The majority of modern test collections are created using a pooling approach, where the union of the top \(n\) documents per topic, taken from a number of different systems, create the pool. Each document in the pool is then judged to be relevant or non-relevant to the topic, and documents which are not in the pool are assumed to be non-relevant.

\cite{Buckley:2007:BLP:1298708.1298735} highlighted a number of issues associated with pooling when dealing with large collections, and showed that standard pooling techniques can introduce bias when dealing with large collections. They make a number of suggestions, such as engineering the topics or forming pools differently, that could reduce bias and allow for the creation of large unbiased test collections using pooling.

\cite{Zobel:1998:RRL:290941.291014} found that although TREC-style pooling produced test collections which give reliable measures of relative performance, there are potential issues with the depth of the pool used, meaning that many relevant documents are not found (30\%-50\%), even in smaller collections. They suggest that pool sizes for each topic should be increased when it is highly likely that further relevant documents will be found.

However, the use of pooling to create a test collection for event detection poses additional challenges which require more substantial changes to the TREC-style pooling methodology. Firstly, the topics are not set in advance, so creating topic-based pools is much more difficult to do.
Each system could detect unique topics (events), meaning that simply taking the top \(n\) documents per system will result in very few of the relevant documents being judged. Secondly, tweets are not ranked against a topic and all tweets are considered equal, meaning that selecting the top \(n\) documents per topic is impossible, so all documents must be judged.

In recent years, crowdsourcing has become a popular method of cheaply and quickly generating relevance judgements.~\cite{Alonso:2008:CRE:1480506.1480508} describe how crowdsourcing can be used to gather relevance judgements. They discuss the advantages of a crowdsourcing methodology called `TERC', which provides fast turnarounds, whilst being low cost and producing high quality results. However, they also describe some of the issues and limitations which need to be addressed, such as quality control and the artificiality of the task. Whilst they propose some solutions to these issues, they recognize that many of the solutions are domain specific, and not applicable in all situations.

Issues with both TREC-style pooling and crowdsourcing highlight the need for a new methodology which can be used for the creation of large-scale test collections for event detection on microblogs. A new type of pooling is needed in order to cope with the lack of predefined topics and document ranking, whilst new crowdsourcing techniques are needed which maintain the fast turnarounds and low-cost associated with crowdsourcing, whilst ensuring high-quality results.