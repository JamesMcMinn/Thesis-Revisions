%!TEX root = ../../main.tex


\section{Collecting Tweets for the Corpus}
\label{sec:methodology}
\label{collection:sec:methodology}
We collected approximately one billion tweets using the Twitter Streaming API\footnote{https://dev.twitter.com/docs/streaming-apis} over a period of 28 days, starting on the 10th of October 2012 and ending on the 7th of November.
This period was chosen specifically because it covers a number of interesting and significant events, including natural disasters such as Hurricane Sandy, and large-scale political events such as the U.S. Presidential Elections.

Since Twitter only allow datasets to be distributed as a list of tweet IDs and requires researchers to crawl the tweets using the Twitter REST API (a time consuming process due to rate limits), we opted to perform a number of basic filters on the corpus to reduce its overall size.
Language filtering was performed using a language detection library for Java\footnote{https://code.google.com/p/language-detection/} to remove non-English tweets.
Further filtering removed common types of Twitter spam (i.e. tweets which contain more than 3 hashtags, more than 3 mentions, or more than 2 URLs, as these have been shown to be strongly associated with spam~\citep{benevenuto2010detecting}).
After spam and language filtering, we were left with a corpus of 120 million tweets.

Of the 120 million tweets in the corpus, around 30\% (40 million) are \emph{retweets}.
A retweet is a copy of another user's tweet which was broadcast by a second user to their followers, often prefixed with `RT'.
In the context of Twitter, retweets are a very useful method of spreading information.
However, retweets are commonly associated with the spread of spam~\citep{5428313}, and because they are an unmodified copy of another user's tweet, they do not generally add any new information.
Although retweets are included in the collection we chose not to include retweets in the relevance judgements.
This allows event detection approaches that make use of retweets to use the collection, whilst reducing the complexity of creating relevance judgements.


\section{Generating Candidate Events using State-of-the-art Event Detection Approaches}
% Event detection in microblogs is very similar to the clustering task~\citep{allan2002topic} (more commonly referred to as \emph{detection}) in TDT.
% In both, a system is presented with a continuous stream of time ordered documents and must place each of them into the most appropriate event-based cluster.
% The main difference between the two tasks is the type and volume of documents in the stream -- however in practice this makes a great deal of difference and event detection on microblog streams is considerably more challenging for a number of reasons.
% Firstly, the volume of documents is several orders of magnitude greater in microblogs, which means that event detection systems must be extremely efficient and preferably able to run in real-time.
% Secondly, the majority of microblog posts discuss mundane, every day occurrences which are not noteworthy or of interest.
% These documents must be filtered out so that only interesting real-world events are detected, an issue which was not present for participants of the TDT evaluations.
% Furthermore, microblog posts tend to be very noisy, of very limited length\footnote{Tweets were restricted to 140 characters at the time the collection was created. Twitter have since increased the limit to 280 characters.} and frequently contain spelling or grammar errors.
% These differences mean that approaches developed for the TDT program tend to be too slow for real-time application, and extremely vulnerable to the noise found in microblog streams.

To generate a set of candidate events for judgment, we choose to use two state-of-the-art detection approaches (at the time of this work was carried out), which were designed specifically for Twitter, namely the Locality Sensitive Hashing approach proposed by~\cite{Petrovic:2010:SFS:1857999.1858020} and the Cluster Summarization approach proposed by~\cite{Aggarwal12}, both of which are described in detail in chapter \ref{chapter:background}.
These were chosen based upon a number of desirable characteristics.
Firstly, both approaches produce clusters of tweets, rather than clusters of terms.
Many event detection approaches for Twitter attempt to reduce the problem by clustering terms rather than documents.
However, this is considerably less useful in our case as they are much more difficult to evaluate and would require an additional step in order to retrieve tweets.
Secondly, both approaches are efficient and we were confident that they would finish in a reasonable time frame (i.e. days rather than weeks or months).
Whilst it would have been desirable to implement additional detection approaches, the time taken to implement, run and evaluate each approach is prohibitive.
Furthermore, since most detection approaches use similar methods to cluster documents and detect events, it is doubtful that a small increase in the number of detection approaches used would significantly increase the coverage of events.

\subsubsection{Locality Sensitive Hashing (LSH)}
Where practical, we used parameters as close as possible to those used by~\cite{Petrovic:2010:SFS:1857999.1858020}.
More precisely, 13 bits per key, a maximum distance of 0.45 and 70 hash tables.
However, we chose to measure the fastest growing clusters on a hourly basis, rather than every 100,000 tweets as used in the original paper.
We made this decision due the fact that 100,000 tweet covers only a short period of time in our collection (approximately 30 minutes) due to its high volume.
Since the LSH approach keeps several hours worth of tweets in memory at all times, this would have generated twice as many candidate events without necessarily increasing the number of real-world events detected, making it prohibitively more expensive to generate judgements.
Simply taking the list of candidate events every hour would still have yielded a prohibitively high number of clusters (in the hundreds of thousands).
However by removing clusters outside the optimal entropy range of 3.5 to 4.25~\citep{Petrovic:2010:SFS:1857999.1858020}, and by removing clusters with fewer than 30 tweets (which ensures the cluster contains enough tweets for effective evaluation), we are left with a final set of 1340 candidate events and a run-time of approximately two days (using desktop class machine with an Intel i5 2500S and 16GB RAM).

\subsubsection{Cluster Summarization (CS)}
We ran the CS approach with 1200 clusters, slightly more than used by \cite{Aggarwal12}, however due to the increased volume of tweets in the collection, we opted to increase the number of clusters so that more topics could be kept in memory.
This gave a reasonable runtime of approximately four days.
Retweets were not used as input to the algorithm as we found that they tended to cause more spam and noise clusters to form and be identified as events.
We used a \(\lambda\) value of 0.0 (i.e. full weight was given to text when calculating similarity) as this greatly decreased the runtime whilst having only a small effect on performance~\citep{Aggarwal12}, and meant that follower information did not have to be crawled, as this would have taken several \emph{years} to obtain via the Twitter API due to rate limits and would have introduced future information.

Similarly to the LSH approach, we removed clusters with fewer than 30 tweets, and those with \(\alpha\) values smaller than 12  (i.e. slow growth rates)~\citep{Aggarwal12}.
Empirically we found that very few clusters with an \(\alpha\) value below 12 discussed events, and by removing these clusters we were able to significantly reduce the number of candidate events to a manageable size of 1097.

\section{Gathering Relevance Judgements for Candidate Events}
This section describes the methodology used to gather relevance judgements for each of the candidate events and their associated tweets.
The objective is to identify which of the 2437 candidate events (1240 from the LSH approach and 1097 from the CS approach) fit our definition of event (i.e. which candidate events are `true events'), and to gather relevance judgements for a sample of the tweets for each true event.
In addition, we also want to gather high-level descriptions and category information for each of the events, which will be useful for merging the events from the different sources (discussed in section~\ref{sec:merge}), and will also be useful as human readable summaries of each event.

The remainder of this section describes the challenges associated with the gathering of relevance judgements for our collection, including how the number of annotators were selected, how much information was gathered using each HIT (Amazon's name for a single crowdsourced job, which stands for `Human Intelligence Task'), and how quality control was performed.

\subsection{Selecting the Number of Annotators}
\label{sec:numAnnotators}
In order to ensure that noise and spam have minimum impact on the evaluations, multiple annotators need to be used for each evaluation.
To help us choose the number of annotators, we ran a pilot using 20 carefully selected candidate events covering a range of categories and with varying degrees of perceived difficulty or ambiguity.
Several candidate events were selected specifically because they were either difficult to judge or fell between event and non-event (i.e. they were very subjective), while other candidates were selected because they contained a mix of relevant and non-relevant tweets, making them more difficult to judge.
Each candidate was annotated by 10 different workers, resulting in 16 of the 20 candidates being identified as an event by a majority (\(k = 0.66\), using Free-marginal multirater kappa~\citep{Randolph}).

It is desirable to minimize the number of annotators per candidate event to reduce the cost and time taken to perform the evaluations, however this must be done carefully and without significantly affecting the quality of the results.
As a minimum, we need 3 annotators for each event to agree and form a majority.
This means that at a minimum, each candidate event needs to be evaluated by 5 annotators.
In order to verify that the reduced number of annotators per event gives similar results to the original 10, we randomly selected 5 evaluations for each of the candidates events and compared the results.
With 5 annotators per candidate, 17 of the 20 candidates were identified as events (\(k = 0.61\)).
Whilst this differs slightly from the results obtained using 10 annotators and there is a very slight drop in overall agreement, it requires only half the number of annotators and greatly reduces costs both financially and in terms of time taken.
Given the similarity of the results using both 5 and 10 annotators, and the fact that 5 annotators still guarantees a minimum agreement of 3 annotators per event, we chose to use 5 annotators for all remaining evaluations.

\subsection{Work per Evaluation}
\label{sec:work}
One of the biggest issues when designing a crowdsourced evaluation is how much work each HIT should entail.
Too much work per HIT and workers will quickly become bored and fatigued, potentially reducing the quality of their annotations.
We decided to try and limit the time taken to perform an individual evaluation to 60 seconds, reducing the chance the workers will become bored or fatigued.
This meant that we could ask only a limited number of questions in each evaluation, and the number of tweets which we could have annotated in each evaluation was also very limited.
In addition to the time taken to answer questions, we also have to consider how much time is required to read the tweets and make a decision about the candidate event (i.e. does the candidate fit our definition of event?), further reducing the amount of questions we could ask in our desired time limit.

Initially, we chose to use 30 tweets, however this caused the time taken to read the tweets and make a judgement about the candidate to be considerably over a minute.
To solve this, we ran empirical evaluations to measure how many tweets we could read in 20 seconds, leaving 40 seconds to answer questions and annotate the tweets.
We found that we were able to carefully read around 13 tweets in a 20 second window, and so used that number for pilots and the evaluation of the detection approaches.

\subsection{Annotating Tweets}
We ran a number of small pilots to test the best method of gathering judgements (i.e. mark relevant, mark non-relevant, or select relevant / non-relevant for each tweet).
Empirically we found that all three options gave similar results, but that selecting relevant / non-relevant for each tweet seemed to give very slightly more accurate results when compared to our own annotations, although these differences were mostly found when annotating subjective tweets.
However, selecting relevant/non-relevant for each tweet is significantly more work than selecting the relevant or non-relevant tweets only, and fatigues annotators much faster than the other methods.
Of the two remaining methods (i.e. selecting only relevant or only non-relevant), we chose to use the selection of non-relevant tweets only as it allows us to more easily use a number of spam detection techniques (described in section~\ref{sec:spam}).

To ensure consistency and quality, the following relevance statement was given to each annotator before the evaluation:
\begin{quote}
	\emph{
Anything that discusses the described event is considered relevant, even if the information is now out-of-date or does not necessarily match that given in other tweets (e.g. the number of deaths is different).
However, care should be taken to mark any untrustworthy or clearly false statements as non-relevant.
Tweets which give a user's opinion of an event, and are obviously discussing the event but do necessarily describe what happened, are still considered relevant.}
\end{quote}

The definition was intentionally very open as we wanted to capture as many tweets about each event as possible.
Specifically, as well as objective tweets, we wanted to include subjective tweets (i.e. the opinion of users) as they are one of the main attractions for using social media data when studying or reporting events, and are generally not found in newswire documents.

\subsection{Annotating a Candidate Event}
\label{collection:sec:annotation}
Before starting their first HIT, annotators were asked to carefully read our definitions of `event' and `significant'.
Each annotator was shown 13 tweets (selected at random, however kept constant between annotators) for a single candidate event.
They were then asked Question 1:
\begin{quote}\emph{``Do the majority of tweets discuss the same real-life topic?''}
\end{quote}
If they answered `no' then the evaluation was complete and they were allowed to submit as this meant that the candidate was \textbf{not} an event.
However, if they answered `yes', they were reminded of our definition of `event' and asked Question 2:
\begin{quote}
\emph{``Do the tweets discuss an event?''}
\end{quote}
Again, annotators who answered `no' were allowed to submit the evaluation as, despite discussing the same topic, the candidate was not an event and there was little point gathering any further information.

Annotators who confirmed that the tweets discussed an event were asked to re-read the tweets and mark any that were non-relevant or off-topic.
Finally, they were asked to provide a brief description of the event and select the most appropriate category.
Rather than ask the annotators to choose between a large number of categories, we chose to use the categories defined and used by the Topic Detection and Tracking (TDT) project~\citep{Allan:2002:ITD:772260.772262}.
The 13 categories defined cover a wide range of topics, with a Miscellaneous category for topics which do not fit elsewhere:

\begin{tabulary}{\textwidth}{l l}
	• Acts of Violence or War & • Celebrity and Human Interest News \\
	• Financial News & • Accidents \\
	• Natural Disasters & • Elections \\
	• Political and Diplomatic Meetings & • Legal / Criminal Cases\\
	• New Laws  & • Scandals / Hearings \\
	• Sports News & • Science and Discovery News \\
	• Miscellaneous News

\end{tabulary}

\subsection{Quality Control}
\label{sec:spam}
During our pilot evaluations, we found that a small number of users were abusing the ability to quickly finish evaluations by answering `no' to Questions 1 or 2.
To make this less appealing, we implemented a 20 second minimum time limit for all evaluations to deter users who simply wanted to spam submission for easy money.
The intuition for this being that users who are only interested quick money will not be willing to wait between successive HITs, and so will not participate in our evaluation.

Despite the minimum time limit, a number of workers continued to abuse the ability to end the evaluation early by answering `no' to either of the first questions.
By examining the ratio of evaluations performed to the number of candidates marked as true events by each user, we were able to identify users who had submitted either exceptionally high numbers of `yes' or `no' answers to the questions \emph{Do the tweets discuss an event?}.
We removed users who had performed over 75 evaluations and had over 90\% `yes' answers or over 90\% `no' answers.
This resulted in the removal of 12 users who had performed 4560 evaluations in total.
This amounts to around 35\% of the total number of evaluations for the detection approaches.
Interestingly, we noted that of the 12 users removed due to spam, 9 appeared in the top 10 users by number of evaluations performed.
This suggests that limiting the number of evaluations which a single user can perform could be a very reliable method of reducing noise and spam.

We also developed several methods of detecting spam submissions so that they can be removed and re-run.
We employed a honey-pot technique to detect users who were not correctly classifying tweets as relevant or non-relevant.
We insert a tweet from a pre-selected set of spam tweets which we know do not discuss an event.
Since the user has already indicated that the tweets they are annotating do discuss an event, we can be sure that a spam tweet is non-relevant.
If the user does not identify this tweet as being non-relevant then their evaluation is marked as being spoiled and we re-run the evaluation with another user.
Of those evaluations submitted as events, only 286 were marked as being spoiled (i.e., the worker failed to identify the honey-pot tweet), which is a good indication that workers were doing a reasonable job of judging relevance.

\subsection{Annotation Results}
Each candidate event was considered to be a true event if more than 50\% of annotators marked it as so, and greater than 90\% of judged tweets for the event were found to be relevant by a majority of annotators.
This resulted in a total of 435 true events: 382 (of 1340) true events for the LSH approach, and 53 (of 1097) for the CS approach.

The choice of 5 annotators for each candidate event was useful for a number of reasons, as we discussed in section~\ref{sec:numAnnotators}, and appears to have been a reasonable choice.
Event agreement increases slightly when 5 annotators are used as opposed to 3 (\(k = 0.91\) and \(k = 0.82\) respectively, using Free-marginal multirater kappa~\citep{Randolph}), whilst agreement at a tweet level remains almost unaffected between 5 and 3 annotators (\(k = 0.91\) and \(k = 0.90\) respectively).
Although this suggests that perhaps 3 annotators would have been sufficient to gather judgements at an event level, it would have resulted in many cases where fewer than 3 annotators judged each tweet, breaking the requirements we outlined in section~\ref{sec:numAnnotators} and reduced overall confidence in the annotations.

\section{The Wikipedia Current Events Portal (CEP)}
Although the use of two different event detection approaches should give us a reasonable sample of events, using events from those systems alone would result in a bias towards easily detectable events.
Wikipedia maintains a Current Events Portal\footnote{http://en.wikipedia.org/wiki/Portal:Current\_events}, which offers a curated set of significant and noteworthy events from the around the world.
The use of Wikipedia offers a number of advantages over the use of additional detection approaches.
Firstly, each of the events listed on the current events portal is substantiated by a link to a news article from a reputable news source.
This allows a high level of confidence that the events are accurate and significant under our definition.
Secondly, much of the work has already been done for us by unpaid editors and is of a high quality, ensured by Wikipedia's editorial guidelines.
This means that we do not have to pay workers to evaluate non-event clusters, reducing the cost and time taken to produce judgements.
In addition, Wikipedia provides complimentary results to the detection approaches thanks to its wide coverage and diversity.

\subsection{Extracting Events using the Wikipedia Current Events Portal}
Each event listed on the Current Events Portal is represented by a description (at most a few sentences), a category, and a link (or links) to relevant news article.
An example event may look similar to:

{
\begin{tabulary}{\textwidth}{r L}
\textbf{Date} & October 25, 2012\\
\textbf{Category} & Business and economics\\
\textbf{Description} & Official [[GDP]] figures indicate the [[2012 Summer Olympic]] helped the [[UK economy]] emerge from recession in the three months from July to September, with growth of 1.0\%. \\
\textbf{Reference} & \texttt{http://www.bbc.co.uk/news/business-20078231}\\
\end{tabulary}
}

Note that words surrounded by [[ and ]] are links to other Wikipedia pages.

Unlike the event detection approaches, we already know that each of the events is a true event, however we still need to identify tweets that discuss each of the events.
First, we must identify a list of candidate tweets.
To do this, we indexed the corpus using Lucene 4.2\footnote{http://lucene.apache.org/}.
Stopword removal was performed, Porter stemming was applied, and prefixes (\#, @ characters) were remove from hashtags and mentions.

We then use the description from each of the Wikipedia events as an initial query to retrieve tweets from the Lucene index.
Query expansion is performed to decrease lexical mismatch, and has been shown to give some of best retrieval performance as part of the TREC Microblog track in 2011~\citep{gambosi2011fub,ferguson2011clarity,li2011pris} and 2012~\citep{kimovercoming,younosFreq,hanhit}.
In particular, we expand links to other Wikipedia pages to the full title of that page (e.g. ``UK economy'' \(\to\) ``Economy of the United Kingdom''), and expand/contract acronyms (e.g. ``U.K.'' \(\to\) ``United Kingdom'', ``United States'' \(\to\) ``U.S.'').
Furthermore, terms used as links to other pages were given double weighting as they are generally the most important and contextual terms in the description.
Divergence from Randomness using Inverse Document Frequency as the basic randomness model was used for retrieval, as experimentation using the TREC11 corpus showed that, of the retrieval models included with Lucene 4.2, it gives the best retrieval performance.

For each of the 367 events on the Wikipedia Current Events Portal listed between the dates covered by the corpus, we retrieved the top 2000 tweets from a window of 72 hours, centred around the day of the event (i.e. for an event on the 16th of October, retrieval was restricted to tweets posted between the 15th and 17th of October inclusively).
The window was used to reduce the probability that tweets discussing similar events would be retrieved, whilst still being long enough to capture any run-up to the event, and at least 24 hours worth of discussion after the event.

Since we only need to gather relevance judgements for tweets, we are able to have more tweets annotated within our desired one minute time limit, allowing us to have each worker annotate 30 tweets per HIT.
Additionally, since we know that every worker will annotate the tweets, we can reduce the number of workers per event from 5 to 3, incurring additional savings.

\subsubsection{Choosing a stopping point}
Whereas tweets from the detection approaches are unranked, tweets obtained using the Wikipedia approach are ranked using the Divergence From Randomness (DFR) \citep{Amati02} retrieval model.
This means that as we progress further down the rankings, the tweets are less likely to be relevant.
Rather than annotate all 2000 tweets for each event from the CEP, we chose to use an incremental approach, inspired by the methodology used by the TDT project, where annotation is stopped once it is unlikely that more relevant tweets will be found.

The ranked list of tweets for each event are split into batches of 30.
Once all annotations are obtained for single batch, a decision is made based upon the number of tweets annotated as relevant by the majority of annotator.
If a batch contains at least 50\% relevant tweets, then the next batch of 30 is submitted for annotation.
On the other hand, if a batch contains less than 50\% relevant tweets, then annotation is stopped for that event and it is marked as complete.
This process is repeated until all events have been marked as complete or all 2000 tweets have been annotated.

In order to determine if our stopping point was effective, we created a pilot study where annotators were shown tweets from after the automatic stopping point (i.e. where there were very few or no relevant tweets).
Surprisingly, the number of tweets marked as relevant by annotators was generally very high, often above 50\%.
We believe that the majority of annotators were confused by the lack of relevant tweets, and created their own pseudo-topic based upon the tweets being shown to them.
For example, where the event described a mass shooting in Nigeria, all 3 annotators seemed to switch to another event, annotating only tweets discussing a bombing at a church in Nigeria as relevant.
Although these events share a similar location, the events themselves are very distinct.
Unfortunately, because of the short length of each tweet, it is easy for a single term to dominate the rankings -- in this case, the only common term between both events was \emph{Nigeria}.
This indicates that continuing to ask for annotations after our stopping point would actually harm the accuracy of results, rather than improve them.

\subsubsection{Annotating the Event}
Annotation for the CEP events was considerably more straightforward than for candidate events from the detection approaches.
For each batch, we asked three annotators to read a description of the event, as taken from the Wikipedia CEP.
We also supplied a link to a relevant news article they could use as a reference.
They were asked to enter a short description of the event that they might use if they were searching for tweets about the event themselves.
Finally, they were shown 30 tweets and asked to mark any non-relevant tweets as so.

The spam and quality control measures, as described in section \ref{sec:spam}, we also used here.
Specifically, a 20 second minimum time limit, and the honey-pot technique to identify low-quality annotators, resulting in 714 being removed (just 4.5\% of the 22,114 total evaluations between both the Wikipedia and Event Detection annotations).
Learning from the results of the previous evaluations, we recorded the number of bad submissions for each user, and any user that failed to identify the honey-pot tweet 3 times was banned from performing further evaluations.

\vspace{-0.5cm}
\subsection{Annotation Results}
Of the 368 events listed on Wikipedia, relevant tweets were found for 361.
In total, 39,980 tweets were annotated as relevant to at least one of the 361 events by more than 50\% of annotators.
Annotator agreement was very high at 0.72, although this is lower than tweet-level agreement for the candidate events generated using the event detection approaches.
It is difficult to say why the agreement is lower for the Wikipedia approach in comparison.
However, we hypothesize that the majority of low quality annotators simply took the easy option for the detection approaches (i.e. answer `no' to either of the first two questions), so never had to judge tweets.
Since there is no ``easy'' option for the Wikipedia evaluations, lazy and low quality annotators are more likely to have a detrimental effect on the quality of the annotations.