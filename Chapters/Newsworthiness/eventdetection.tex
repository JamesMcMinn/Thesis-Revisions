%!TEX root = ../../main.tex

\section{Newsworthiness as a Feature for Event Detection}
\label{scoring:sec:detection}

One of the aims of this work is to determine if newsworthiness can be incorporated into an existing event detection approach and whether it can be used as a feature to improve effectiveness.
In this section, we examine how newsworthiness can be combined with a simple entity-based approach and used to filter out noisy clusters or clusters with a low Newsworthiness Score, improving event detection effectiveness.

The entity-based event detection approach described in chapter \ref{detection:chapter} provides a reasonable base approach which can be built upon.
Since the aim is to evaluate the effectiveness of our Newsworthiness Score as a feature to identify newsworthy content, we ignore all non-clustering features used by the entity-based approach, such as burst detection.
Without burst detection there is no way to combine clusters into larger events, so we report each cluster as an individual event.

\subsubsection{Cluster Newsworthiness}

We define the Newsworthiness Score \(N\) of cluster \(c\) as the mean Newsworthiness Score of each tweet \(d\) in the cluster:
\begin{displaymath}
	N_c = \frac{\sum{N_d}}{D}
\end{displaymath}
where \(D\) is the total number of tweets in the cluster.

\subsection{Evaluation}
\label{scoring:sec:detectionEval}

Cosine is used to measure the distance between tweets, and each tweet is clustered with its nearest neighbour within a distance of 0.5.
Although stopword removal and stemming are not used to calculate Newsworthiness Scores, we do stem and remove stopwords for clustering.

Clustering is performed in a streaming manner, and individual tweet newsworthiness is calculated in real-time, however mean cluster newsworthiness is calculated post-hoc, so results presented here do not represent a true streaming situation.
However, the aim is to demonstrate that newsworthiness can be used as an effective feature for event detection, rather than to develop a novel real-time event detection technique.
In practice, calculating the mean Newsworthiness Score for a cluster in real-time is trivial, and any difference in performance is likely to be small, however we note the difference for accuracy.

As we evaluate individual clusters as separate events, precision values reported here are likely to be lower than if similar clusters were combined using either the event merging approach from chapter \ref{chapter:collection} or by grouping using burst detection.
This is due to an increase in the number of small clusters evaluated, which increases the likelihood that a newsworthy and event-relevant cluster will be evaluated as non-relevant due to the incomplete relevance judgements.

\subsubsection{Results}
Table \ref{scoring:table:detection} shows Precision and Recall values at for clusters of various minimum Newsworthiness Scores and sizes.
The top row, marked Any, shows baseline figures without any restrictions on cluster Newsworthiness, while other rows show precision and recall values after removing any clusters with Newsworthiness Scores below the filter value.

\begin{table}[h]
	\centering
	\caption{Newsworthiness scores using different term models for tweets known to be relevant to a newsworthy event against the average score for all other tweets in the collection.}
	\begin{tabulary}{\textwidth}{l | c c | c c | c c | c c}
		\toprule

		 \textbf{} & \multicolumn{2}{c|}{\textbf{5 Tweets}} & \multicolumn{2}{c|}{\textbf{30 Tweets}} & \multicolumn{2}{c|}{\textbf{50 Tweets}} & \multicolumn{2}{c}{\textbf{100 Tweets}} \\

		\textbf{Score} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} \\

		\midrule
		\textbf{Any} & 0.010 & 0.755 & 0.053 & 0.587 & 0.075 & 0.508 & 0.121 & 0.399 \\
		\textbf{≥ 0} & 0.016 & 0.715 & 0.093 & 0.540 & 0.134 & 0.464 & 0.220 & 0.354 \\
		\textbf{≥ 1} & 0.019 & 0.690 & 0.108 & 0.518 & 0.156 & 0.439 & 0.254 & 0.326 \\
		\textbf{≥ 2} & 0.023 & 0.607 & 0.123 & 0.439 & 0.175 & 0.356 & 0.290 & 0.243 \\
		\textbf{≥ 3} & 0.033 & 0.466 & 0.154 & 0.298 & 0.220 & 0.227 & 0.366 & 0.136 \\
		\textbf{≥ 4} & 0.040 & 0.132 & 0.248 & 0.055 & 0.374 & 0.030 & 0.585 & 0.020 \\

		\bottomrule
	\end{tabulary}
	\label{scoring:table:detection}
\end{table}

A clear trend can be see as both the minimum cluster size and minimum cluster Newsworthiness Score are increased; recall drops, but precision increases. With a minimum cluster size of 5, a maximum recall of 0.755 is achieved, however measured precision is very low at only 0.010.
As we increase the minimum cluster Newsworthiness Score, recall drops off, however precision increases significantly.
Although F1 scores are not shown, increasing the minimum Newsworthiness score towards 3 increases the F1 measure for all cluster sizes, however after 3, overall F1 score begins to decrease.

Although a fully crowdsourced evaluation would provide a detailed picture of how our Newsworthiness Scoring approach performs when used as a feature for event detection, it goes beyond the scope of this chapter.
Instead, we perform a small manual evaluation focused on determining the precision of our Newsworthiness Scoring approach.
We chose to evaluate clusters using a similar methodology to that used to create the Events 2012 corpus, however used only a single expert rather than 5 crowdsourced workers.
We selected 2 sets of clusters to evaluate:
\begin{itemize}
	\item 100 randomly selected clusters (from a total of 4,270) with 5 or more tweets and a cluster Newsworthiness Score ≥ 4.
	\item All 115 clusters with 50 or more tweets and a cluster Newsworthiness score ≥ 4.
\end{itemize}

Of the 100 randomly selected clusters from 4,270 with 5 or more tweets, 95/100 clusters were marked as a significantly real-world event, giving a precision of 0.950.
Since there were only 115 clusters with 50 or more tweets, we evaluated all 115, rather than take a sample of 100. All 115 were marked as significant real-world events, giving a precision of 1.0.
Both of these results far exceed the automatically calculated precision values of 0.040 and 0.347 respectively.
The reasons for this are similar to those described in chapter \ref{detection:chapter}; although the collection has relevance judgements for a relatively large number of events, it does not cover all events during the month long period the collection covers, or even all tweets for the 506 events it has judgements for.

These manually obtained results vastly outperform even the crowdsourced results presented in chapter \ref{detection:chapter} and suggest that Newsworthiness could be used as an extremely effective noise filter, requiring only 5 tweets for extremely high precision, and obtaining perfect precision at only 50 tweets.
The automatically measured performance values are comparable to that of our entity-based event detection approach, despite being considerably simpler and being disadvantaged due to our use of clusters for evaluation rather than full events, as noted in section \ref{scoring:sec:detectionEval}.