%!TEX root = ../../main.tex

\section{Experimentation}
\label{detection:sec:experimentation}
We ran our entity-based approach on the Events 2012 collection we created in chapter \ref{chapter:collection}, treating it as a time-ordered stream.
Named Entity Recognition and Part of Speech Tagging was performed using the Stanford Core NLP Toolkit\footnote{http://www-nlp.stanford.edu/software/tagger.shtml} (version 3.2.0).
Unless otherwise stated, all runs were performed using the top 10 IDF-weighted terms per tweet, a maximum of 500 tweets were retrieved per term, and a minimum cosine similarity of 0.5.

Although the collection contains 150,000 relevance judgements for 506 events, we note that it does not cover all events that occurred during the 28 days it covers, and that it is also likely that it does not have full relevance judgements for discussing events it does cover.
This does not prevent us from using the collection for comparison purposes, however it means that event precision and recall can only be estimated.
Whilst this is an issue in many IR collections, we note that the effect is more pronounced when dealing with event detection as, unlike most IR tasks, there is no query.
This makes it likely that we will detect events, or parts of events, that are not in the judgements.
We verify this hypothesis and show that we detect a large number of events which are not in the relevance judgements by performing a crowdsourced evaluation.

\subsection{Automatic Evaluations}
To perform systematic, automatic evaluations, we must make changes to the standard relevance judging process to account for the incomplete relevance judgements.
Rather than requiring that every tweet matches for a candidate event to be determined a true-positive, we use a threshold based judgment process.
The threshold must be both low enough to enable partial matches, but still identity any false-positives.
Too high a threshold will make it difficult for an event to be found relevant if the system being evaluated operates at a different level of granularity from the relevance judgements or has only detected part of the event.
For example, a full football match rather than an individual goal, or grouping many small but related events into a `super-event', such as Hurricane Sandy causing damage as it moves across the United States.
Too low a threshold could result in many false-positives and misleading results.

Empirically, we found that if 5\% or 15 tweets (whatever the smallest is) can be matched from a candidate event to an event in the relevance judgements, then is is almost always correctly labeled as a true-positive.
Precision and recall can then be calculated on an event basis using the standard precision and recall metrics given in chapter \ref{chapter:background}.
While this may seem like a low threshold, empirically, we found that it produces very few false-positives (i.e. non-relevant events being identified as relevant), whilst allowing for a great deal of flexibility in terms of event granularity.

Automatic Evaluations were performed on all events with more than 30 tweets, using a 5\% threshold and/or minimum of 15 matching tweets.
Any variation from these thresholds is noted alongside the results presented in section \ref{detection:sec:results}.

\subsection{Crowdsourced Evaluation}
\label{sec:baseline}
\label{detection:sec:crowd}
Given that no event detection technique for Twitter has been robustly evaluated against a publicly available Twitter collection, the only options available to us as a baseline are the LSH \citep{Petrovic10} and CS \citep{Aggarwal12} approaches used to generate the collection in chapter \ref{chapter:collection}.
This means that the results are extremely biased towards these approaches, and only a crowdsourced evaluation will allow for a direct comparison between our entity-based approach and the baselines.
As we use the results from the crowdsourced evaluation, baseline parameters have not changed from those used in chapter \ref{collection:chapter}.

In order to keep the comparison between the baselines and our approach as fair as possible, we use the same methodology here as we did in chapter \ref{chapter:collection} to gather relevance judgements for the baseline approaches.
5 annotators were used per event, and to keep category classification fair, we used the same 13 categories defined by the Topic Detection and Tracking project \citep{Allan:2002:ITD:772260.772262}.
To generate the results that follow, we mapped the TDT categories back to the 8 categories used by the collection.
A number of spam and quality control measures were used, identical to those described in chapter \ref{chapter:collection}.

\subsubsection{Crowdsourced Evaluation Limitations}
Since resources were limited, and our approach generates a large number of candidate events, we use a random sample of 250 events from the 1210 candidate events identified by our approach.
We can then estimate precision across all 1210 events by taking the number of true-positives found in the 250 crowdsourced evaluations and interpolating up to 1210.
For example, if 159 of the 250 events are found to be true-positives after crowdsourcing ($P = 0.636$), then we can estimate that 769 of all 1210 events are true-positives ($1210 \times 0.636 = 769$).

Recall is much more difficult to estimate and poses and number of issues.
If our entity-based approach discovers many new events, leading to a much higher precision score using crowdsourcing compared to automatic evaluations, we cannot be certain how many of these are new events or if we are simply lacking some relevance judgements for part of an event already in the collection.
This is a particular problem for large events that may have many thousands, or in some cases, millions of tweets, discussing them, such as the U.S. Presidential Debates or Hurricane Sandy.
Whilst the event merging approach we developed in chapter \ref{chapter:collection} may seem like an obvious solution, there are a number of issues that prevent us from using it to calculate recall.
Although we are able to estimate precision, we do not know which events are true-positives and which are false-positives, other than the 250 that were evaluated using crowdsourcing.
Without first removing all false-positives, we cannot accurately merge events and determine how many new events have been detected.
To do so would require a full crowdsourced evaluation of all 1210 events, a prohibitively costly and time intensive task, despite having all the tools and infrastructure already in place.

This means that recall for the crowdsourced evaluation must still be based off of events already in the collection's relevance judgements, and that the estimated recall will always under-measure the true recall.