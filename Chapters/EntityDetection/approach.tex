%!TEX root = ../../main.tex

\section{Entity-based Event Detection Approach}
\label{detection:sec:approach}
In this section we describe our entity-based event detection approach.
The approach comprises of 6 key stages, as shown in Figure \ref{detection:graphic:pipeline}.
Tweets are processed in order using a pipelines architecture that allows for parallel processing, with each component relying only on the output of the previous component to complete its task.

\vspace{0.5cm}
\begin{figure}[h!]
	\centering
	\includegraphics[width=11cm,trim=1.5cm 1cm 1.5cm 0.8cm]{Chapters/EntityDetection/images/system.pdf}
	\caption{The pipeline architecture and components of our entity-based approach.}
	\label{detection:graphic:pipeline}
\end{figure}

\subsection{Tweet Pre-processing}
Our approach uses a number of pre-processing steps.
This helps to filter out unwanted tweets, such as common types of spam, and attempts to provide a noise free stream of tweets which can used to detect events.

\subsubsection{Parsing \& Tagging}
\label{detection:sec:parsing}
We use the Stanford CoreNLP Natural Language Toolkit to perform Part of Speech (POS) tagging and Named Entity Recognition (NER) on the text of each tweet.
We  use the GATE Twitter POS model~\citep{TwitterPOS} for Part of Speech tagging, and the Stanford 3 class model for Named Entity Recognition.
The GATE Twitter POS model is a Part-of-speech models trained specifically on tweets and tends to provide much high accuracy ($>$ 90\%) that part-of-speech models trained on non-twitter corpora.

We extract all nouns, verbs and named entities from each tweet.
Nouns and verbs are lemmatized, and entities are kept in their longest form to ensure that names are as distinguishing as possible  (i.e. ``Paul Ryan'' rather than ``Paul'' and ``Ryan'').

The Stanford 3 class NER model is able to perform automatic class-based disambiguation such that entity mentions are also give a class (person, location, or organization).
We use this class information and consider named entities of different classes to be distinct, even if the name itself is identical.
For example, the entity ``Spain'' in the context of the football team (an \emph{organization} consisting on many people that play football) is conceptually different from ``Spain'' the (the landmass, a \emph{location}), as this helps to retain as much distinguishing power as possible.
We evaluate and give reasoning for this choice in section \ref{detection:sec:entityTypes}.
All terms and entities are converted to lowercase and any non-alphanumeric characters are removed (however whitespace is retained in the case of named entities).

\subsubsection{Filtering}
Event detection is analogous to a filtering task in many ways -- by removing as many non-event related tweets as possible, we are more likely to find event related ones. To this end, we apply a set of filters which remove over 95\% of tweets.
This has a number of benefits.
Firstly, assuming that the filters remove more noise than signal, it becomes considerably easier to extract events.
Secondly, unlike other approaches which filter after clustering~\citep{Petrovic:2010:SFS:1857999.1858020,becker2011beyond}, filtering before clustering reduces the amount of data which needs to be processed, and plays a significant role in the ability of our approach to detect events in a time and space efficient manner.

We first remove tweets that contain no named entities.
This is our most aggressive filter, removing over 90\% of tweets.
As discussed in section \ref{detection:sec:entityEvents}, we believe that named entities play a crucial role in describing events, thus do not believe that this filter significantly harms detection performance, and provide some evidence towards this in section \ref{detection:sec:entitiesEval}.
Furthermore, in order to efficiently cluster tweets, our clustering approach (described in section~\ref{detection:sec:clustering}) requires each tweet to contain at least one named entity, making this filter necessary.

We also remove retweets, which make up approximately 30\% of all tweets and are simply a copy of an existing tweet.
Retweets require little effort to produce, meaning that they are often associated with the spread of spam, memes and misinformation~\citep{Grier:2010:SUC:1866307.1866311}, and do not provide any new information.
We examine the effect of removing retweets in section \ref{detection:sec:retweetsEval}, and show that their removal improves precision.

We also use a range of term-level filters that remove terms and entities that are unlikely to be related to an event or that are known to be associated with spam and noise.
As well as stop words and expletives, we remove terms associated with watching television (``watch'', ``film'',``movie'', ``episode'', etc.) or listening to music (``listen'', ``song'', ``play'', etc.).
This helps to reduce the number of false positives created by large numbers of users watching a television show while using a ``second screen'' to discuss it, as significant, but fictional events in television shows can often cause reactions that appear similar to real-world events.
We also remove terms and entities associated with traditional news and broadcast agencies, such as ``bbc news'', ``cnn'', ``fox news'' and ``reuters''.
Tweets from these agencies often contain their own name, which can cause issues with our entity-based clustering and merging approaches (described in sections \ref{detection:sec:clustering} and \ref{detection:sec:entityLinking}) as the same entity can appear to be associated with a large number of events simultaneously.
Term level filters such as these are likely to require some maintenance as the usage of Twitter continues to evolve and require some domain knowledge to construct, however this small amount of expert work helps to remove vast amounts of noise.
Finally, terms under 3 characters in length are removed.

\subsection{Entity-based Online Clustering}
\label{detection:sec:clustering}
Almost all event detection approaches make use of single pass nearest neighbor clustering to find documents that discuss the same topic or event, similar to those used by the TDT project described in section \ref{background:alg:tdt}.
As we have already discussed, these approaches get slower as the number of documents grows, eventually causing them to become too slow for high volume streams like \citep{Petrovic10}.
A number of solutions have been proposed to solve this, including the use of Locality Sensitive Hashing~\citep{Petrovic10}, which we described in section \ref{background:sec:lsh}.
However, these approaches are general purpose, and do not make use of any domain or task specific information.
Given the role that named entities play in describing real-world events, we believe it makes sense to use their presence to improve clustering efficiency and effectiveness for the task of event detection.

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Minimum similarity threshold $m$}
	index $\gets [][]$ \\
	clusters $\gets$ [][] \\

	\ForEach{tweet d in the stream} {

		\ForEach{entity e in d} {

			$S(d) \gets \emptyset$ \tcp*{set of documents that share a term with $d$}
			\ForEach{non-entity term t in d} {
				\ForEach{tweet d' in index[e][t]} {
					$S(d) \gets S(d) \cup d'$ \\
				}
				index[$e$][$t$] $\gets$ index[$e$][$t$] $\cup$ $d$ \\
			}

			$c_{max} \gets 0$ \tcp*{maximum cosine between $d$ and tweets in $S(d)$}
			$n_{d} \gets nil$ \tcp*{tweet with maximum cosine to $d$}
			\ForEach{tweet d' in S(d)}{
				$c$ := cosine($d$, $d'$) \\
				\If{$c > c_{max}$}{
					$c_{max} \gets c$ \\
					$n_d \gets d'$
				}
			}

			\eIf{$c_{max} \geq m$} {
				add $d$ to clusters[$e$][$n_d$]
			}{
				\tcp{Unlike most approaches, we do not assume a new cluster is a new event}
				clusters[$e$][$d$] $\gets$ new cluster($d$)
			}

		}
	}

\caption{Entity-based method of clustering}
\label{detection:alg:clustering}
\end{algorithm}

Algorithm \ref{detection:alg:clustering} shows the pseudocode for our entity-based clustering approach.
Using the premise that tweets discussing an event must contain at least one of the named entities involved in the event, we adapt the traditional TDT clustering model by partitioning tweets based upon the entities they contain, and add a tweet to a cluster for every entity it contains, as show in in Figure \ref{detection:graphic:clustering}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=8cm,trim=0cm 1.5cm 0cm 0cm]{Chapters/EntityDetection/images/clustering.pdf}
	\caption[Entity-based clustering]{Tweets are added to a cluster for each entity they contain. The example shows how a tweet containing both `Obama' and `Romney' would be put into two clusters, one for the entity `Obama' and one for the entity `Romney'.}
	\label{detection:graphic:clustering}
\end{figure}

For the purpose of clustering, this can be thought of as having a unique inverted index per named entity that the system has seen.
For each named entity $e$ in tweet $d$, a list of tweets $D$ is retrieved from the inverted index for $e$ and the maximum TF-IDF weight cosine similarity score is calculated between $d$ and each tweet in $D$.
The new tweet is then added to the inverted index for entity $e$.

To ensure that our approach is able to run in real-time, we limit the number of tweets that can be retrieved from an entity's inverted index to a fixed number per term (usually between 100-1000), and use only the top 10 TF-IDF weighed terms per tweet.
Less than 1\% of tweets from our test collection contain more than 10 terms, meaning that we lose very little information by enforcing this limit, whilst ensuring an upper bound of \(10N\) comparisons are made, where \(N\) is the maximum number of documents retrieved per term.
Nouns, verbs and named entities are used when calculating the cosine similarity between tweets.
The named entity whose inverted index was used to retrieve comparison documents is given a weight of 0 to give as much weight as possible to other topical terms.

If the maximum score is above a set threshold (usually in the range \(0.4-0.6\) \citep{Petrovic10}, discussed in section \ref{detection:sec:simThresholds}), then $d$ is added to the same cluster as its nearest neighbour, or when a nearest neighbor cannot be found within the threshold, a new cluster is created containing only $d$.

At a very high level, our clustering approach has similarities to those used by many TDT approaches, in that it performs single-pass nearest neighbor clustering and makes use of an inverted index.
However, the similarities end there, and unlike TDT, we do not assume that the formation of a new cluster indicates a new event.
The remainder of this section describes how our approach decides that a new real-world event has occurred, and how clusters relating to those events are identified.

\subsection{Identifying Bursty Entities}
For an effective event detection approach, it is important to have a method of detecting significant events and filtering the mundane.
We do this by looking for temporal bursts in the frequency of an entity, which can occur over periods ranging from a few minutes to several hours.
To model this, we use a set of windows for each entity to capture their frequency over time, starting at 5 minutes, and doubling in length up to 320 minutes (i.e. 5, 10, 20, \ldots, 320).

We use the Three Sigma Rule as the basis for a light-weight burst detection approach, which states that a value is considered to be statistically unlikely if it is further than 3 standard deviations from the expected value \citep{Pukelsheim94}.
A similar approach is used by \cite{Aggarwal12} to determine an minimum similarity threshold for their clustering approach described in section \ref{background:cs}.

\begin{algorithm}[t]
	\caption{Efficient computation of $\mu$ and $\sigma$ values for multiple windows and entities}
	\label{detection:alg:updatingEntityValues}

	\DontPrintSemicolon

	lengths $\gets$ [5, 10, 20, 40, 80, 160, 320]\\
	$M_0, M_1, M_2 \gets$ [][], [][], [][] \\
	$\mu, \sigma \gets$ [][], [][] \\
	bursting $\gets$ [] \\
	burstUntil $\gets$ [] \\

	\ForEach{tweet d in the stream} {

		\ForEach{entity e in d} {

			\If{bursting[$e$] is true and burstUntil[$e$] $< t$}{
				bursting[$e$] $\gets$ false \tcp*{mark the entity as finished bursting}
				burstUntil[$e$] $\gets$ 0\\
			}

			\ForEach{window w in lengths} {
				$c \gets$ number of times entity $e$ has appered in last $w$ seconds \\
				\tcp{Update window statistics if enough time has passed}
				\If{seconds since last statistics update for window $\geq$ lengths[$w$]}{
					$M_0 \gets M_0 + 1$ \\
					$M_1 \gets M_1$ + $c$ \\
					$M_2 \gets M_2$ + $c^2$ \\
					$\mu$[$e$][$w$] $\gets M_1 / M_0$ \\
					$\sigma$[$e$][$w$] $\gets \sqrt{(M_0M_2 - M_1^2) / M_0} $ \\
				}

				\If{$c > \mu[e][w] + (3 \cdot \sigma[e][w])$}{
					bursting[$e$] $\gets$ true 				\tcp*{mark the entity as bursting}
					burstUntil[$e$] $\gets t + (w \times 1.5)$ \\
				}
			}

		}
	}
\end{algorithm}

For each window, we maintain mean and standard deviation values, updating them periodically with the current entity frequency.
It is possible to efficiently compute moving mean (\(\mu\)) and standard deviation (\(\sigma\)) values using a set of three power sums \(M_0\), \(M_1\) and \(M_2\), for vector $E$ containing counts of how frequently entity $e$ is mentioned during each window:
\begin{align*}
	M_0 &= \sum_{i = 0}^{|E|}{E_i^0} & M_1 &= \sum_{i = 0}^{|E|}{E_i^1} & M_2 &= \sum_{i = 0}^{|E|}{E_{i}^2}
\end{align*}
The current mean and standard deviation values (\(\mu\) and \(\sigma\) respectively), can then calculated as so:
\begin{equation*}
	\begin{aligned}[c]
		\mu = \frac{M_1}{M_0}
	\end{aligned}
	\qquad\qquad
	\begin{aligned}[c]
		\sigma = \sqrt{\frac{M_0 M_2 - M_1^2}{M_0}}
	\end{aligned}
\end{equation*}

The \(\mu\) and \(\sigma\) values are computed for each named entity and each window length, requiring $|e| \cdot 7 \cdot 3$ integer values to be stored, where $|e|$ is the number of named entities seen by the system, 7 is the number of windows, and 3 represents the $M_0$, $M_1$ and $M_2$ values.
Entity frequencies are updated periodically based upon the length of the window (i.e., a 5 minute window is updated every 5 minutes).
Algorithm \ref{detection:alg:updatingEntityValues} gives the pseudocode for this approach.

Once a tweet has been clustered and added to entity indexes, we update the entity statistics, and check if the newly added tweet caused the entity to burst in any of the windows.
If the number of tweets in a given window is greater than \(\mu + (3 \cdot \sigma)\), then we say that the entity is bursting.

In order to smooth and reduce noise, statistics are not updated while a window is bursting, and windows are kept in a bursting state for \(1.5 \times window\_length\) after the window's statistics suggest that it has stopped bursting.
For example, an entity which is marked as bursty in the 80 minute window would remain bursting for 120 minutes after the 80 minute window stops bursting.
The extra time allows for fluctuations in frequency as an event develops, and helps prevent a state of flux where an entity rapidly switches between bursting and normal states, particularly in the early stages of an event where the overall volume of discussion may still be quite low.

When a tweet which is no longer covered by the largest window it is removed from all inverted indexes.
This helps to reduce the number of comparisons required when calculating the nearest neighbour, and allows resources to be freed for incoming tweets.
We do not believe that removing older tweets will affect the effectiveness of the algorithm due to the real-time nature of Twitter, as tweets which are more than a few hours old are unlikely to b relevant to any ongoing real-world events, and those which are will hopefully have already been marked as so by the algorithm, ensuring that they are recorded elsewhere.

\subsection{Event Creation and Cluster Selection}
\label{detection:sec:eventCreation}
Once a burst has been detected (lines 20-24 in algorithm \ref{detection:alg:updatingEntityValues}), we have potentially detected an event.
However, we must first identify a set of clusters that discuss the event before reporting it as a new event.
We cannot simply take all clusters containing tweets posted during the burst as these will contain background topics about the bursting entity, such as discussion about visiting a location or listening to a particular artist's music.
To solve this, we propose the use of a number of heuristics to select clusters that are the most likely to be related to the event that caused the burst.

We require that the centroid time of a cluster (i.e. the average timestamp associated with all tweets in a cluster) is greater than \(B_e\), where \(B_e\) is the time at which the entity began to burst.
This helps to ensure that clusters which discuss background topics are not included as they are likely to have existed for some time before the burst took place.
A cluster's centroid time is updated as new tweets are added, ensuring that clusters which initially had a centroid time prior to the burst can still be added to an event.
This is important as it allows clusters containing early reports of the event, which often occur before any burst takes places, to be included.

We also require that a cluster meets a minimum size threshold, usually between 5 and 20 tweets (10 is used for our evaluations).
This is to ensure that the cluster covers a significant portion of the event and to prevent small but noisy clusters from being included. The minimum size has an effect on the precision of our approach, and large minimum cluster sizes can give a large increase in precision for a small reduction in recall.
We discuss the minimum cluster size in detail in section \ref{detection:sec:simThresholds}.

If at least one cluster meets the time and size requirements outlines above, then it is added to the event for the entity to which it belongs, or, if it is the first cluster to be identified after a burst, then it is added to a new event.
An event is kept as long as it has at least one bursting entity associated with it.
Once all entities associated with an event have stopped bursting, the event is finalized, and no more clusters or tweets can be added to it.

\subsection{Entity-Event Linking and Merging}
\label{detection:sec:entityLinking}
New events are related to only a single entity.
However, most events involve more than one entity, such as a person and a location, or an interaction between two organizations.
In additional, events are often discussed using a number of synonyms.
For example ``Barack Obama'' is often shortened to simply ``Obama''.
These issues mean that there are generally a number of events, each created by separate named entities, which discuss the same real-world event.
Entity disambiguation tools, such as TagMe, can perform entity disambiguation and may help for common cases, such as mentions of significant people.
However, their reliance on existing ontologies such as Wikipedia mean that there will always be a lag between an event occurring and that information being updated \citep{WRN2012:osbornebieber}, which makes them less useful for rapidly developing real-world events.
Our approach relies on no external sources of information, and is able to generate links between entities and events in real-time based using entity co-occurrence statistics.

Formally, if an named entity $n$ is mentioned in at least 50\% of tweets in an event $e$, then we say there is a link between the event $e$ and the entity $n$.
If entity $n$ is bursting and has an event $e'$, then events $e$ an $e'$ and merged to create event $e''$.
This process is can be repeated many times, and any entity which is mentioned in event $e''$ by more than 50\% of tweets is said to have a link to event $e''$, and so on until no more links are found.
This process is repeated as new tweets or clusters are added to the event.

If an entity stops bursting, then any links to an event are ended, and no new clusters from the entity can be added to the event.
However, tweets added to clusters that are already part of the event are still added to the event.
An events continues to be updated until all linked entities have stopped bursting and all clusters in the event have become inactive (i.e. all the of the tweets in the event's clusters have been removed from the inverted index associated with its entity).
This helps to capture more information about the event, even after discussion has died down.

Note that the 50\% requirement is uni-directional, not bi-directional, and only one of the two events needs to meet the 50\% requirement to link the events.
This allows less frequently mentioned entities to be merged into large events, which often discuss a diverse range of sub-topics, making it difficult for any single entity to be mentioned in 50\% of tweets.
For example, during one of the U.S. Presidential Debates between `Obama' and `Romney', there were a wide range of topics discussed.
It is unlikely that any single topic, such as `China', would be mentioned in the majority of tweets, making it impossible to form a link if the 50\% requirement was bi-directional.
It is however likely that an event for the `China' sub-topic will mention `Obama' or `Romney' in the majority of tweets, allowing it to be merged into the main event for the debate.

\subsubsection{Entity Normalization}
\label{sec:entityNorm}
As described in section \ref{detection:sec:clustering}, entities are kept in their longest form rather than split into individual components (e.g. `Barack Obama', rather than `Barack' and `Obama').
However, this can cause issues as multiple events are created for different forms of the same entity.
Using our previous example, it is unlikely that single tweet will mention both `Barck Obama' and `Obama', resulting in two separate events that our entity linking approach is unlikely to merge.

To solve this, we perform a naive entity normalization technique.
When measuring the frequency of entity mentions within an event, we perform a normalization step, which counts the last term of people and organization names as a separate entity.
For example, for event mention of `barack obama', we also increase the count of `obama'.
Note that we only do this for People and Organizations, but not Locations, as locations tend to lose much of their meaning when split (e.g. United Kingdom $\rightarrow$ Kingdom).